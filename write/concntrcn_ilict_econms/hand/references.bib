@article{SRVR:auto_11_2020,
	author = {{SRVR}},
	title = {Auto 11 de 2020, Sala de Reconocimiento de Verdad y Responsabilidad y Determinación los Hechos y Conductas},
	journal = {Jurisdicción Especial para la Paz},
	volume = {},
	number = {},
	year={2020},
	pages = {},
	doi = {}
}

@article{SRVR:auto_134_2021,
	author = {{SRVR}},
	title = {Auto 134 de 2021, Sala de Reconocimiento de Verdad y Responsabilidad y Determinación los Hechos y Conductas},
	journal = {Jurisdicción Especial para la Paz},
	volume = {},
	number = {},
	year={2020},
	pages = {},
	doi = {}
}

@article{informe:2022,
	author = {{JEP-CEV-HRDAG}},
	title = {Informe metodológico del proyecto conjunto JEP-CEV-HRDAG de integración de datos y estimación estadística},
	journal = {Comisión para el Esclarecimiento de la Verdad, la Convivencia y la No Repetición},
	volume = {},
	number = {},
	year={2022},
	pages = {200},
	doi = {},
	url = {https://www.comisiondelaverdad.co/anexo-proyecto-jep-cevhrdag}
}

@article{enamorado:2019, 
  author={Enamorado, Ted and Fifield, Benjamin and Imai, Kosuke}, 
  title={Using a Probabilistic Model to Assist Merging of Large-Scale Administrative Records}, 
  volume={113}, 
  DOI={10.1017/S0003055418000783}, 
  number={2}, 
  journal={American Political Science Review}, 
  publisher={Cambridge University Press}, 
  year={2019}, 
  pages={353–371}
  }


@article{lst,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}
@book{gelman:2013,
  title={Bayesian Data Analysis, Third Edition},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={Chapman and Hall/CRC}
}
@book{rubin:1987,
  added-at = {2009-10-28T04:42:52.000+0100},
  author = {Rubin, Donald B},
  biburl = {https://www.bibsonomy.org/bibtex/20a72dbca78fbd5bc8ba192f31f5d5f2f/jwbowers},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {ec0d6b5ac7fe288d46c91a4158ed0777},
  intrahash = {0a72dbca78fbd5bc8ba192f31f5d5f2f},
  keywords = {Bayesian Missing Sample approach data; survey;},
  publisher = {Wiley},
  timestamp = {2009-10-28T04:43:19.000+0100},
  title = {Multiple Imputation for Nonresponse in Surveys},
  year = 1987
}
@article{white:2011,
	author = {White, Ian R and Royston, Patrick and Wood, Angela M},
	title = {Multiple imputation using chained equations: Issues and guidance for practice},
	journal = {Statistics in Medicine},
	volume = {30},
	number = {4},
	pages = {377-399},
	keywords = {missing data, multiple imputation, fully conditional specification},
	doi = {https://doi.org/10.1002/sim.4067},
	abstract = {Abstract Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments. Copyright © 2010 John Wiley \& Sons, Ltd.},
	year = {2011}
}
@article{he_et_al:2010,
	author = {Y. He and AM Zaslavsky and MB Landrum and DP Harrington and P. Catalano},
	title ={Multiple imputation in a large-scale complex survey: a practical guide},
	journal = {Statistical Methods in Medical Research},
	volume = {19},
	number = {6},
	pages = {653-670},
	year = {2010},
	doi = {10.1177/0962280208101273},
		note ={PMID: 19654173},
	URL = { https://doi.org/10.1177/0962280208101273 },
	eprint = { https://doi.org/10.1177/0962280208101273
	},
		abstract = { The Cancer Care Outcomes Research and Surveillance (CanCORS) Consortium is a multisite, multimode, multiwave study of the quality and patterns of care delivered to population-based cohorts of newly diagnosed patients with lung and colorectal cancer. As is typical in observational studies, missing data are a serious concern for CanCORS, following complicated patterns that impose severe challenges to the consortium investigators. Despite the popularity of multiple imputation of missing data, its acceptance and application still lag in large-scale studies with complicated data sets such as CanCORS. We use sequential regression multiple imputation, implemented in public-available software, to deal with non-response in the CanCORS surveys and construct a centralised completed database that can be easily used by investigators from multiple sites. Our work illustrates the feasibility of multiple imputation in a large-scale multiobjective survey, showing its capacity to handle complex missing data. We present the implementation process in detail as an example for practitioners and discuss some of the challenging issues which need further research. }
}
@book{vanbuuren:2018,
	author = "Stef {van Buuren}",
	title = "Flexible Imputation of Missing Data",
	year = "2018",
	publisher = "Chapman and Hall/CRC",
	address = "Boca Raton",
	URL = "https://stefvanbuuren.name/fimd"
}
@article{burgette:2010,
	author = {Lane F Burgette and Jerome P Reiter},
	year = {2010},
	title = {Multiple Imputation for Missing Data via Sequential Regression Trees},
	journal = {American Journal of Epidemiology},
	volume = {172},
	number = {9},
	pages = {1070-–6}
}
@mastersthesis{akande:2015,
	author = {Olanrewaju Michael Akande},
	title = {A Comparison Of Multiple Imputation Methods For
Categorical Data},
	school = "Duke University",
	year = "2015"
}
@article{JSSv045i03,
   author = {Stef {van Buuren} and Karin Groothuis-Oudshoorn},
   title = {mice: Multivariate Imputation by Chained Equations in R},
   journal = {Journal of Statistical Software, Articles},
   volume = {45},
   number = {3},
   year = {2011},
   keywords = {},
   abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
   pages = {1--67},
   doi = {10.18637/jss.v045.i03}
}
@article{kropko:2014,
	title = "Multiple imputation for continuous and categorical data: Comparing joint multivariate normal and conditional approaches",
	abstract = "We consider the relative performance of two common approaches to multiple imputation (MI): joint multivariate normal (MVN) MI, in which the data are modeled as a sample from a joint MVN distribution; and conditional MI, in which each variable is modeled conditionally on all the others. In order to use the multivariate normal distribution, implementations of joint MVN MI typically assume that categories of discrete variables are probabilistically constructed from continuous values. We use simulations to examine the implications of these assumptions. For each approach, we assess (1) the accuracy of the imputed values; and (2) the accuracy of coefficients and fitted values from a model fit to completed data sets. These simulations consider continuous, binary, ordinal, and unordered-categorical variables. One set of simulations uses multivariate normal data, and one set uses data from the 2008 American National Election Studies. We implement a less restrictive approach than is typical when evaluating methods using simulations in the missing data literature: in each case, missing values are generated by carefully following the conditions necessary for missingness to be {"}missing at random{"} (MAR). We find that in these situations conditional MI is more accurate than joint MVN MI whenever the data include categorical variables.",
	author = "Jonathan Kropko and Ben Goodrich and Andrew Gelman and Jennifer Hill",
	year = "2014",
	doi = "10.1093/pan/mpu007",
	language = "English (US)",
	volume = "22",
	pages = "497--519",
	journal = "Political Analysis",
	publisher = "Oxford University Press",
	number = "4",
}
@article{chao:1987,
	author = {Anne Chao},
	title = {Estimating the Population Size for Capture-Recapture Data with Unequal Capturability},
	journal = {Biometrics},
	year = {1987},
	volume = {43},
	pages = {783--791}
}
@article{chao:2001a,
  title={The applications of capture-recapture models to epidemiological data},
  author={Chao, Anne and Tsay, PK and Lin, Sheng-Hsiang and Shau, Wen-Yi and Chao, Day-Yu},
  journal={Statistics in medicine},
  volume={20},
  number={20},
  pages={3123--3157},
  year={2001},
  publisher={Wiley Online Library}
}
@article{chao:2001b,
	author = {Anne Chao},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	title = {An Overview of Closed Capture-Recapture Models},
	volume = {6},
	number = {2},
	year = {2001},
	pages = {158--175}
}
@article{madigan:1997,
 URL = {http://www.jstor.org/stable/2337552},
 abstract = {A Bayesian methodology for estimating the size of a closed population from multiple incomplete administrative lists is proposed. The approach allows for a variety of dependence structures between the lists, can make use of covariates, and explicitly accounts for model uncertainty. Interval estimates form this approach are compared to frequentist and previously published Bayesian approaches. Several examples are considered.},
 author = {David Madigan and Jeremy C York},
 journal = {Biometrika},
 number = {1},
 pages = {19--31},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Bayesian Methods for Estimation of the Size of a Closed Population},
 volume = {84},
 year = {1997}
}
@article{amoros:2014,
	title = {Recapturing Laplace},
	author = {Jaume Amorós},
	journal = {Significance},
	year = {2014},
	URL = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2014.00754.x}
}
@article{scheuren:2004,
	title = {History Corner},
	author = {Fritz Scheuren},
	journal = {The American Statistician},
	year = {2004},
	URL = {https://www.tandfonline.com/doi/abs/10.1198/0003130042926}
}
@book{bishop:1975,
    author = {Yvonne M M Bishop and Stephen E Fienberg and Paul W Holland},
    title = {Discrete Multivariate Analysis: Theory and Practice},
    year = {1975},
	publisher = {Springer}
}
@article{cormack:1989,
 URL = {http://www.jstor.org/stable/2531485},
 abstract = {Log-linear models are developed for capture-recapture experiments, and their advantages and disadvantages discussed. Ways in which they can be extended, sometimes with only partial success, to open populations, subpopulations, trap dependence, and long chains of recapture periods are presented. The use of residual patterns, and analysis of subsets of data, to identify behavioural patterns and acceptable models is emphasised and illustrated with two examples.},
 author = {R M Cormack},
 journal = {Biometrics},
 number = {2},
 pages = {395--413},
 publisher = {[Wiley, International Biometric Society]},
 title = {Log-Linear Models for Capture-Recapture},
 volume = {45},
 year = {1989}
}
@article{sekar:1949,
	title = {On a Method of Estimating Birth and Death Rates and the Extent of Registration},
	author = {C Chandra Sekar and W Edwards Deming},
	journal = {Journal of the American Statistical Association},
	volume = {44},
	number = {245},
	year = {1949},
	URL = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1949.10483294}
}
@article{fienberg:1999,
  author={Stephen E Fienberg and M S Johnson and B W Junker},
  title={{Classical multilevel and Bayesian approaches to population size estimation using multiple lists}},
  journal={Journal of the Royal Statistical Society Series A},
  year=1999,
  volume={162},
  number={3},
  pages={383-405},
  month={},
  keywords={},
  doi={10.1111/1467-985X.00143},
  abstract={One of the major objections to the standard multiple‐recapture approach to population estimation is the assumption of homogeneity of individual ‘capture’ probabilities. Modelling individual capture heterogeneity is complicated by the fact that it shows up as a restricted form of interaction among lists in the contingency table cross‐classifying list memberships for all individuals. Traditional log‐linear modelling approaches to capture–recapture problems are well suited to modelling interactions among lists but ignore the special dependence structure that individual heterogeneity induces. A random‐effects approach, based on the Rasch model from educational testing and introduced in this context by Darroch and co‐workers and Agresti, provides one way to introduce the dependence resulting from heterogeneity into the log‐linear model; however, previous efforts to combine the Rasch‐like heterogeneity terms additively with the usual log‐linear interaction terms suggest that a more flexible approach is required. In this paper we consider both classical multilevel approaches and fully Bayesian hierarchical approaches to modelling individual heterogeneity and list interactions. Our framework encompasses both the traditional log‐linear approach and various elements from the full Rasch model. We compare these approaches on two examples, the first arising from an epidemiological study of a population of diabetics in Italy, and the second a study intended to assess the ‘size’ of the World Wide Web. We also explore extensions allowing for interactions between the Rasch and log‐linear portions of the models in both the classical and the Bayesian contexts.}
}

@article{darroch:1993,
 URL = {http://www.jstor.org/stable/2290811},
 abstract = {A central assumption in the standard capture-recapture approach to the estimation of the size of a closed population is the homogeneity of the "capture" probabilities. In this article we develop an approach that allows for varying susceptibility to capture through individual parameters using a variant of the Rasch model from psychological measurement situations. Our approach requires an additional recapture. In the context of census undercount estimation, this requirement amounts to the use of a second independent sample or alternative data source to be matched with census and Post-Enumeration Survey (PES) data. The models we develop provide a mechanism for separating out the dependence between census and PES induced by individual heterogeneity. The resulting data take the form of an incomplete 23 contingency table, and we describe how to estimate the expected values of the observable cells of this table using log-linear quasi-symmetry models. The projection of these estimates onto the unobserved cell corresponding to those individuals missed by all three sources involves the log-linear model of no second-order interaction, which is quite plausible under the Rasch model. We illustrate the models and their estimation using data from a 1988 dress-rehearsal study for the 1990 census conducted by the U.S. Bureau of the Census, which explored the use of administrative data as a supplement to the PES. The article includes a discussion of extensions and related models.},
 author = {John N Darroch and Stephen E Fienberg and Gary F V Glonek and Brian W Junker},
 journal = {Journal of the American Statistical Association},
 number = {423},
 pages = {1137--1148},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {A Three-Sample Multiple-Recapture Approach to Census Population Estimation with Heterogeneous Catchability},
 volume = {88},
 year = {1993}
}
@article{hoovergreen:2019,
 URL = {https://www.jstor.org/stable/26850667},
 author = {Amelia {Hoover Green} and Patrick Ball},
 journal = {Demographic Research},
 number = {},
 pages = {781--814},
 publisher = {Max-Planck-Gesellschaft zur Foerderung der Wissenschaften},
 title = {Civilian killings and disappearances during civil war in El Salvador (1980–1992)},
 volume = {41},
 year = {2019}
}
@article{johndrow:2019,
    author = {James E Johndrow and Kristian Lum and Daniel Manrique-Vallier},
    title = "{Low-risk population size estimates in the presence of capture heterogeneity}",
    journal = {Biometrika},
    volume = {106},
    number = {1},
    pages = {197-210},
    year = {2019},
    abstract = {Population estimation methods are used for estimating the size of a population from samples of individuals. In many applications, the probability of being observed in the sample varies across individuals, resulting in sampling bias. We show that in this setting, estimators of the population size have high and sometimes infinite risk, leading to large uncertainty in the population size. As an alternative, we propose estimating the population of individuals with observation probability exceeding a small threshold. We show that estimators of this quantity have lower risk than estimators of the total population size. The proposed approach is shown empirically to result in large reductions in mean squared error in a common model for capture-recapture population estimation with heterogeneous capture probabilities.},
    doi = {10.1093/biomet/asy065},
}
@article{link:2004,
	title = {Individual heterogeneity and identifiability in capture recapture models},
	author = {William A Link},
	journal = {Animal Biodiversity and Conservation},
	volume = {27},
	number = {1},
	pages = {87--91},
	year = {2004}
}
@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}
@article{lum2013applications,
  title={Applications of multiple systems estimation in human rights research},
  author={Lum, Kristian and Price, Megan and Banks, David},
  journal={The American Statistician},
  volume={67},
  number={4},
  pages={191--200},
  year={2013},
  publisher={Taylor \& Francis}
}
@article{gohdes2010different,
  title={Different convenience samples, different stories: The case of Sierra Leone},
  author={Gohdes, Anita},
  year={2010}
}
@article{manrique2016bayesian,
  title={Bayesian population size estimation using Dirichlet process mixtures},
  author={Manrique-Vallier, Daniel},
  journal={Biometrics},
  volume={72},
  number={4},
  pages={1246--1254},
  year={2016},
  publisher={Wiley Online Library}
}
@article{bird2018multiple,
  title={Multiple systems estimation (or capture-recapture estimation) to inform public policy},
  author={Bird, Sheila M and King, Ruth},
  journal={Annual review of statistics and its application},
  volume={5},
  pages={95--118},
  year={2018},
  publisher={Annual Reviews}
}
@article{ball2019using,
  title={Using statistics to assess lethal violence in civil and inter-state war},
  author={Ball, Patrick and Price, Megan},
  journal={Annual review of statistics and its application},
  volume={6},
  pages={63--84},
  year={2019},
  publisher={Annual Reviews}
}
@article{ball2016registro,
  title={El registro y la medici{\'o}n de la criminalidad. El problema de los datos faltantes y el uso de la ciencia para producir estimaciones en relaci{\'o}n con el homicidio en Colombia, demostrado a partir de un ejemplo: el departamento de Antioquia (2003-2011)},
  author={Ball, Patrick and Reed Hurtado, Michael},
  journal={Revista Criminalidad},
  volume={58},
  number={1},
  pages={9--23},
  year={2016}
}
@article{ball-and-reed-2015,
	title = {Exploración y análisis de los datas para comprender la realidad},
	author = {Ball, Patrick and Reed Hurtado, Michael},
	year = {2015},
	journal = {Forensis 2014},
	number = {1},
	volume = {16},
	pages = {529-545},
	publisher = {Instituto Nacional de Medicina Legal y Ciencias Forenses}
}
@article{Fellegi1969,
  author = {Fellegi, Ivan P and Sunter, Alan B},
  title = {{A Theory for Record Linkage}},
  journal = {Journal of the American Statistical Association},
  year = {1969},
  volume = {64},
  number = {328},
  pages = {1183--1210},
  url = {http://www.jstor.org/stable/2286061}
}
@article{Newcombe1959,
  author = {Newcombe, H. B. and Kennedy, J. M. and Axford, S. J. and James, A. P.},
  title = {{Automatic Linkage of Vital Records}},
  journal = {Science},
  year = {1959},
  volume = {130},
  number = {3381},
  pages = {954--959},
  url = {http://links.jstor.org/sici?sici=0036-8075%2819591016%293%3A130%3A3381%3C954%3AALOVR%3E2.0.CO%3B2-T}
}
@article{Dunn1946,
  author = {Dunn, Halbert L},
  title = {{Record Linkage}},
  journal = {American Journal of Public Health},
  year = {1946},
  volume = {36},
  number = {12},
  pages = {1412--1416},
  doi = {10.2105/AJPH.36.12.1412},
}
@article{ChenG16,
  author    = {Tianqi Chen and Carlos Guestrin},
  title     = {XGBoost: {A} Scalable Tree Boosting System},
  journal   = {CoRR},
  volume    = {abs/1603.02754},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.02754},
  archivePrefix = {arXiv},
  eprint    = {1603.02754},
  timestamp = {Mon, 13 Aug 2018 16:47:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChenG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{michelson-aaai06,
  author = {Matthew Michelson and Craig A Knoblock},
  title = {Learning Blocking Schemes for Record Linkage},
  booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence (AAAI)},
  year = {2006}
}
@inproceedings{ChristenVatsalanWang2015,
	author = {Peter Christen and Dinusha Vatsalan and Qing Wang},
	title = {Efficient Entity Resolution with Adaptive and Interactive Training Data Selection},
	year = {2015},
	url = {http://users.cecs.anu.edu.au/~u5170295/papers/icdm-christen-2015.pdf},
	booktitle = {2015 {{IEEE}} International Conference on Data Mining},
	abstract = {Entity resolution (ER) is the task of deciding which records in one or more databases refer to the same real-world entities. A crucial step in ER is the accurate classification of pairs of records into matches and non-matches. In most practical ER applications, obtaining training data is costly and time consuming. Various techniques have been proposed for ER to interactively generate training data and learn an accurate classifier. We propose an approach for training data selection for ER that exploits the cluster structure of the weight vectors (similarities) calculated from compared record pairs. Our approach adaptively selects an optimal number of informative training examples for manual labeling based on a user defined sampling error margin, and recursively splits the set of weight vectors to find pure enough subsets for training. We consider two aspects of ER that are highly significant in practice: a limited budget for the number of manual labeling that can be done, and a noisy oracle where manual labels might be incorrect. Experiments on four real public data sets show that our approach can significantly reduce manual labeling efforts for training an ER classifier while achieving matching quality comparative to fully supervised classifiers.}
}
@book{Herzog-book,
  author = {Thomas N Herzog and Fritz J Scheuren and Wiliam E Winkler},
  title = {{Data Quality and Record Linkage Techniques}},
  year = 2007,
  publisher = {Springer},
  isbn = {978-0-387-69502-0},
  url = {http://www.springer.com/computer/database+management+%26+information+retrieval/book/978-0-387-69502-0}
}
@inproceedings{BilenkoKamathMooney2006,
	author = {Mikail Bilenko and Beena Kamath and Raymond J Mooney},
	title = {Adaptive Blocking: Learning to Scale Up Record Linkage},
	publisher = {Proceedings of the 6th IEEE International Conference on Data Mining},
	year = {2006},
	url = {http://bilenko.com/papers/06-icdm.pdf}
}
@book{Christen2012,
  author = {Peter Christen},
  title = {Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection},
  year = {2012},
  address = {New York},
  publisher = {Springer}
}
@article{mullner2013fastcluster,
  title={fastcluster: Fast hierarchical, agglomerative clustering routines for R and Python},
  author={M{\"u}llner, Daniel},
  journal={Journal of Statistical Software},
  volume={53},
  number={9},
  pages={1--18},
  year={2013},
  publisher={Foundation for Open Access Statistics}
}
@inproceedings{10.1145/2939672.2939785,
	author = {Chen, Tianqi and Guestrin, Carlos},
	title = {XGBoost: A Scalable Tree Boosting System},
	year = {2016},
	isbn = {9781450342322},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {785–794},
	numpages = {10},
	keywords = {large-scale machine learning},
	location = {San Francisco, California, USA},
	series = {KDD '16}
}
@article{ball2018statistics,
  title={The statistics of genocide},
  author={Ball, Patrick and Price, Megan},
  journal={CHANCE},
  volume={31},
  number={1},
  pages={38--45},
  year={2018},
  publisher={Taylor \& Francis}
}
@misc{angel2019asesinatos,
  title={Asesinatos de lideres sociales en Colombia: una estimaci{\'o}n del universo-Actualizaci{\'o}n 2018},
  author={Rozo {\'A}ngel, Valentina and Ball, Patrick},
  year={2019},
  publisher={Human Rights Data Analysis Group}
}
@article{elliot:2017,
author = {Michael R Elliott and Richard Valliant},
title = {{Inference for Nonprobability Samples}},
volume = {32},
journal = {Statistical Science},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {249 -- 264},
abstract = {Although selecting a probability sample has been the standard for decades when making inferences from a sample to a finite population, incentives are increasing to use nonprobability samples. In a world of “big data”, large amounts of data are available that are faster and easier to collect than are probability samples. Design-based inference, in which the distribution for inference is generated by the random mechanism used by the sampler, cannot be used for nonprobability samples. One alternative is quasi-randomization in which pseudo-inclusion probabilities are estimated based on covariates available for samples and nonsample units. Another is superpopulation modeling for the analytic variables collected on the sample units in which the model is used to predict values for the nonsample units. We discuss the pros and cons of each approach.},
keywords = {coverage error, Hierarchical regression, quasi-randomization, reference sample, selection bias, superpopulation model},
year = {2017},
doi = {10.1214/16-STS598},
URL = {https://doi.org/10.1214/16-STS598}
}
@article{gao:2021,
author = {Yuxiang Gao and Lauren Kennedy and Daniel Simpson and Andrew Gelman},
title = {{Improving Multilevel Regression and Poststratification with Structured Priors}},
volume = {-1},
journal = {Bayesian Analysis},
number = {-1},
publisher = {International Society for Bayesian Analysis},
pages = {1 -- 26},
abstract = {A central theme in the field of survey statistics is estimating population-level quantities through data coming from potentially non-representative samples of the population. Multilevel regression and poststratification (MRP), a model-based approach, is gaining traction against the traditional weighted approach for survey estimates. MRP estimates are susceptible to bias if there is an underlying structure that the methodology does not capture. This work aims to provide a new framework for specifying structured prior distributions that lead to bias reduction in MRP estimates. We use simulation studies to explore the benefit of these prior distributions and demonstrate their efficacy on non-representative US survey data. We show that structured prior distributions offer absolute bias reduction and variance reduction for posterior MRP estimates in a large variety of data regimes.},
keywords = {bias reduction, integrated nested Laplace approximation (INLA), multilevel regression and poststratification, non-representative data, small-area estimation, Stan, structured prior distributions},
year = {2021},
doi = {10.1214/20-BA1223},
URL = {https://doi.org/10.1214/20-BA1223}
}
@article{rivest:2011,
author = {Louis-Paul Rivest},
title = {{A Lower Bound Model for Multiple Record Systems Estimation withHeterogeneous Catchability}},
volume = {7},
journal = {The International Journal of Biostatistics},
number = {1},
year = {2011},
doi = {10.2202/1557-4679.1283}
}
@thesis{cro:2017,
	author = {Cro, S},
	year = {2007},
	title = {Relevant Accessible Sensitivity Analysis for Clinical Trials with Missing Data},
	school = "PhD thesis, London School of Hygiene & Tropical Medicine",
	year = "2017"
}
@book{heymans:2019,
  title={Applied Missing Data Analysis With SPSS and (R)Studio},
  author={Heymans, M and Eekhout, I},
  year={2019},
  publisher={Heymans and Eekhout}
}
@article{zhou:2010,
	title = {A Note on Bayesian Inference After Multiple Imputation},
	author = {Zhou, X and Reiter, J.},
	journal = {The American Statistician},
	year = {2010},
	volume = {64},
	number = {2}
}
@article{chao:2015,
  title={Capture-Recapture for Human Populations},
  author={Chao, Anne},
  journal={Wiley StatsRef: Statistics Reference Online},
  pages={1--16},
  year={2015},
  publisher={John Wiley \& Sons, Ltd Chichester, UK}
}
@article{ignatieff:1996,
  title={Articles of Faith},
  author={Ignatieff, Michael },
  journal={Index On Censorship },
  year={1996},
  vol={5}
}
@book{Borges:1944,
  title={El jardín de los senderos que se bifurcan},
  author={Borges, Jorge Luis},
  year={1994},
  publisher={Ficciones}
}
@article{rubin:1996,
	author = {Rubin, Donald B},
	title = {Multiple imputation after 18 plus years},
	journal = {Journal of the American Statistical Association},
	volume = {91},
	number = {434},
	year={1996},
	pages = {473 - 489},
	year = 1996
}
@article{ball:2016,
	title = {The Task Is A Quantum Of Workflow},
	author = {Ball, Patrick},
	webpage = {HRDAG},
	year = {2016},
	URL = {https://hrdag.org/2016/06/14/the-task-is-a-quantum-of-workflow/}
}
@book{ball:2008,
	title = {Quién le hizo qué a quién},
	author = {Ball, Patrick},
	publisher={Benetech},
	year = {2008}
}
@article{rubin:1978,
	author = {Rubin, Donald B},
	title = {Multiple Imputations in Samples Surveys - A Phenomenological Bayesian Approach to Nonresponse.},
	journal = {American Statistical Association},
	volume = {Proceedings of the Survey Research Methods Section},
	year={1996},
	pages = {20 -- 34},
	year = 1978
}
@article{price:2015,
	author = {Price, M., Gohdes, A. and Ball, P.},
	title = {Documents of war: Understanding the Syrian conflict.},
	journal = {Royal Statistical Society},
	volume = {12},
	year={2015},
	pages = {14 -- 19},
	year = 2015
}
@article{gelman:2014,
	author = {Gelman, Andrew and Loken, E.},
	title = {The Statistical Crisis in Science},
	journal = {American Scientist},
	year={2014},
}
@book{sardan:2003,
  title={Model Assisted Survey Sampling},
  author={Särndal, K.E., Swensson, B., Wretman, J. },
  year={2003},
  publisher={Springr}
}
@article{mv:2020,
	author = {Manrique-Vallier, Daniel and Ball, P., and Sadinle, M.},
	title = {Capture-Recapture for Casualty Estimation and Beyond: Recent Advances and Research Directions},
	book = {American Scientist},
	year={2022},
	publisher={Springr}
}
@article{shah:2019,
	title = {.Rproj considered harmfu},
	author = {Shah, T.},
	webpage = {HRDAG},
	year = {2019},
	URL = {https://hrdag.org/tech-notes/harmful.html}
}
@book{hagberg:2008,
author = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
title = {Exploring Network Structure, Dynamics, and Function using NetworkX},
booktitle = {Proceedings of the 7th Python in Science Conference},
pages = {11 - 15},
address = {Pasadena, CA USA},
year = {2008},
editor = {Ga"el Varoquaux and Travis Vaught and Jarrod Millman},
}
@article{gutierrez:2017,
	title = {What Should We Mean by “Pattern of Political Violence”? Repertoire, Targeting, Frequency, and Technique},
	author = {Gutiérrez-Sanín, F. and Wood, E.},
	journal = {Cambridge Core},
	volume = {15},
	year = {2017}
}
@article{Berchenk:2009,
  title = {Emergence and Size of the Giant Component in Clustered Random Graphs with a Given Degree Distribution},
  author = {Berchenko, Yakir and Artzy-Randrup, Yael and Teicher, Mina and Stone, Lewi},
  journal = {Phys. Rev. Lett.},
  volume = {102},
  issue = {13},
  pages = {138701},
  numpages = {4},
  year = {2009},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.102.138701},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.102.138701}
}
@article{Li:2021,
  title = {Percolation on complex networks: Theory and application},
  author = {Ming Li, Run-Ran Liu, Linyuan Lü, Mao-Bin Hu, Shuqi Xu and Yi-Cheng Zhang},
  journal = {Physics Reports},
  volume = {907},
  pages = {1-68},
  year = {2021}
}
@article{Johndrow:2018,
  title = {Theoretical limits of microclustering for record linkage},
  author = {Johndrow, James and Lum, Kristian and Dunson, David},
  journal = {Biometrika},
  volume = {105},
  pages = {431-446},
  year = {2018}
}
@article{suarez:2018,
  title = {Dos registros, dos versiones. Los usos políticos de los registros oficiales en las luchas por la verdad de la guerra en Colombia},
  author = {Andrés Felipe Suárez},
  journal = {Nuevo Mundo Mundos Nuevos},
  year = {2018}
}

# done
